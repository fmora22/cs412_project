{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/fymor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/fymor/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/fymor/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Cleaning\n",
    "In this section, we will clean the dataset by:\n",
    "\n",
    "- Removing missing values.\n",
    "- Removing URLs, emojis, and punctuation.\n",
    "- Lowercasing all text for consistency. \n",
    "- Removing duplicates\n",
    "\n",
    "The goal is to ensure the dataset is clean and ready for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data Overview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = \"cyberbullying_tweets.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#  the first few rows of the dataset\n",
    "print(\"Initial Data Overview:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      "tweet_text            0\n",
      "cyberbullying_type    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values in 'tweet_text' column\n",
    "df.dropna(subset=['tweet_text'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define emoji removal pattern\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    u\"\\U00002700-\\U000027BF\"  # dingbats\n",
    "    u\"\\U00002600-\\U000026FF\"  # misc symbols\n",
    "    u\"\\U0001F900-\\U0001F9FF\"  # supplemental symbols & pictographs\n",
    "    u\"\\U0001FA70-\\U0001FAFF\"  # extended pictographs\n",
    "    \"]+\", flags=re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define cleaning function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by:\n",
    "    - Lowercasing\n",
    "    - Removing URLs, emojis, and punctuation\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "\n",
    "    # Remove emojis\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    # Handle punctuation\n",
    "    text = re.sub(r\"\\'s\", '', text)\n",
    "    text = re.sub(r\"'\", '', text)\n",
    "    text = re.sub(r\"[\" + string.punctuation + \"]\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'str'>]\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning function\n",
    "df['cleaned_text'] = df['tweet_text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# Verify cleaned_text contains strings\n",
    "print(df['cleaned_text'].apply(type).unique())  # Should only show <class 'str'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate Tweets:\n",
      "2147\n",
      "\n",
      "Duplicate Tweets After Dropping:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate tweets\n",
    "print(\"\\nDuplicate Tweets:\")\n",
    "print(df.duplicated(subset=['cleaned_text']).sum())\n",
    "\n",
    "# Drop duplicate tweets\n",
    "df.drop_duplicates(subset=['cleaned_text'], inplace=True)\n",
    "\n",
    "# Check duplicates again after dropping\n",
    "print(\"\\nDuplicate Tweets After Dropping:\")\n",
    "print(df.duplicated(subset=['cleaned_text']).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Tokenization and Lemmatization\n",
    "Here we tokenize the text into words and lemmatize each word to its root form for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for tokenization and lemmatization\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Converts Treebank POS tags to WordNet POS tags.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lemmatize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes and lemmatizes cleaned text.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Processed tokens\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    pos_tags = nltk.pos_tag(tokens)  # Part-of-speech tagging\n",
    "    lemmatizer = WordNetLemmatizer()  # Initialize lemmatizer\n",
    "    processed_tokens = [\n",
    "        lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag))\n",
    "        for token, tag in pos_tags\n",
    "        if token not in string.punctuation and token not in stop_words\n",
    "    ]\n",
    "    return processed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization and lemmatization\n",
    "df['processed_tokens'] = df['cleaned_text'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "# Save the processed dataset\n",
    "df.to_csv('processed_tweets.csv', index=False)\n",
    "\n",
    "# save into a new df\n",
    "cleaned_df = pd.read_csv('processed_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and Verifying the Processed Data\n",
    "Finally, we save the cleaned and tokenized dataset to a new CSV file for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Processed Data:\n",
      "                                    processed_tokens\n",
      "0       [word, katandandre, food, crapilicious, mkr]\n",
      "1  [aussietv, white, mkr, theblock, imacelebritya...\n",
      "2  [xochitlsuckkks, classy, whore, red, velvet, c...\n",
      "3  [jason, gio, meh, p, thanks, head, concern, an...\n",
      "4  [rudhoeenglish, isi, account, pretend, kurdish...\n"
     ]
    }
   ],
   "source": [
    "# Display sample processed tokens\n",
    "print(\"\\nSample Processed Data:\")\n",
    "print(df[['processed_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Exploratory Data Analysis (EDA)\n",
    "In this section we will analyze the class distribution to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution:\n",
      "cyberbullying_type\n",
      "religion               7967\n",
      "age                    7940\n",
      "ethnicity              7913\n",
      "not_cyberbullying      7860\n",
      "gender                 7734\n",
      "other_cyberbullying    6131\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClass Distribution:\")\n",
    "print(df['cyberbullying_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Frequency:\n",
      "    Token  Frequency\n",
      "0   bully      10793\n",
      "1  school       9233\n",
      "2       â€™       9003\n",
      "3    fuck       7336\n",
      "4    like       6183\n",
      "5  nigger       5765\n",
      "6     get       5530\n",
      "7       u       5513\n",
      "8    joke       5506\n",
      "9    girl       5478\n"
     ]
    }
   ],
   "source": [
    "# Token frequency (flat list of all tokens)\n",
    "from collections import Counter\n",
    "all_tokens = [token for tokens in df['processed_tokens'] for token in tokens]\n",
    "token_counts = Counter(all_tokens).most_common(10)\n",
    "#print as a table\n",
    "print(\"\\nToken Frequency:\")\n",
    "print(pd.DataFrame(token_counts, columns=['Token', 'Frequency']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Feature Engineering\n",
    "Transform processed_tokens into numerical features suitable for model input:\n",
    "Bag-of-Words (BoW). \n",
    "or \n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) ( i belive this is what we selected in our proposal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Modeling\n",
    "train and test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Model Evaluation\n",
    "Compare models using evaluation metrics like:\n",
    "Accuracy.\n",
    "Precision, Recall, and F1-Score.\n",
    "Confusion matrix for deeper insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Stretch Goals (Optional)\n",
    "can cross-compare it with our base model\n",
    "\n",
    "- Sentiment Analysis: Pre-trained sentiment analysis - models (e.g., using TextBlob or VADER) can be applied to processed_text.\n",
    "    - Compare the effectiveness of sentiment-only features against other feature engineering methods (e.g., Bag-of-Words or embeddings).\n",
    "    - Test if sentiment adds value to the regular model's performance.\n",
    "- Intent Detection: Use embeddings or transformer models for intent classification (if time allows)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
