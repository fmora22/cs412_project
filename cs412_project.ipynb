{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/fymor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/fymor/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/fymor/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Cleaning\n",
    "In this section, we will clean the dataset by:\n",
    "\n",
    "- Removing missing values.\n",
    "- Removing URLs, emojis, and punctuation.\n",
    "- Lowercasing all text for consistency. \n",
    "- Removing duplicates\n",
    "\n",
    "The goal is to ensure the dataset is clean and ready for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data Overview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = \"cyberbullying_tweets.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#  the first few rows of the dataset\n",
    "print(\"Initial Data Overview:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      "tweet_text            0\n",
      "cyberbullying_type    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values in 'tweet_text' column\n",
    "df.dropna(subset=['tweet_text'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define emoji removal pattern\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    u\"\\U00002700-\\U000027BF\"  # dingbats\n",
    "    u\"\\U00002600-\\U000026FF\"  # misc symbols\n",
    "    u\"\\U0001F900-\\U0001F9FF\"  # supplemental symbols & pictographs\n",
    "    u\"\\U0001FA70-\\U0001FAFF\"  # extended pictographs\n",
    "    \"]+\", flags=re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by:\n",
    "    - Lowercasing\n",
    "    - Removing URLs, emojis, punctuation, and special characters\n",
    "    - Removing single-character tokens\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002700-\\U000027BF\"  # dingbats\n",
    "        u\"\\U00002600-\\U000026FF\"  # misc symbols\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # supplemental symbols & pictographs\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # extended pictographs\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r\"â€™\", '', text)  # Handle smart apostrophes\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text)  # Remove all punctuation except word characters and spaces\n",
    "\n",
    "    # Remove single-character tokens (except meaningful ones like 'a', 'i')\n",
    "    text = re.sub(r'\\b[b-z]\\b', '', text)  # Exclude 'a' and 'i' from removal\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'str'>]\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning function\n",
    "df['cleaned_text'] = df['tweet_text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# Verify cleaned_text contains strings\n",
    "print(df['cleaned_text'].apply(type).unique())  # Should only show <class 'str'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate Tweets:\n",
      "2176\n",
      "\n",
      "Duplicate Tweets After Dropping:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate tweets\n",
    "print(\"\\nDuplicate Tweets:\")\n",
    "print(df.duplicated(subset=['cleaned_text']).sum())\n",
    "\n",
    "# Drop duplicate tweets\n",
    "df.drop_duplicates(subset=['cleaned_text'], inplace=True)\n",
    "\n",
    "# Check duplicates again after dropping\n",
    "print(\"\\nDuplicate Tweets After Dropping:\")\n",
    "print(df.duplicated(subset=['cleaned_text']).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Tokenization and Lemmatization\n",
    "Here we tokenize the text into words and lemmatize each word to its root form for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for tokenization and lemmatization\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Converts Treebank POS tags to WordNet POS tags.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lemmatize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes and lemmatizes cleaned text.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Processed tokens\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    pos_tags = nltk.pos_tag(tokens)  # Part-of-speech tagging\n",
    "    lemmatizer = WordNetLemmatizer()  # Initialize lemmatizer\n",
    "    processed_tokens = [\n",
    "        lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag))\n",
    "        for token, tag in pos_tags\n",
    "        if token not in string.punctuation and token not in stop_words\n",
    "    ]\n",
    "    return processed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization and lemmatization\n",
    "df['processed_tokens'] = df['cleaned_text'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "# Save the processed dataset\n",
    "df.to_csv('processed_tweets.csv', index=False)\n",
    "\n",
    "# save into a new df\n",
    "cleaned_df = pd.read_csv('processed_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and Verifying the Processed Data\n",
    "Finally, we save the cleaned and tokenized dataset to a new CSV file for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Processed Data:\n",
      "                                    processed_tokens\n",
      "0       [word, katandandre, food, crapilicious, mkr]\n",
      "1  [aussietv, white, mkr, theblock, imacelebritya...\n",
      "2  [xochitlsuckkks, classy, whore, red, velvet, c...\n",
      "3  [jason_gio, meh, thanks, head, concern, anothe...\n",
      "4  [rudhoeenglish, isi, account, pretend, kurdish...\n"
     ]
    }
   ],
   "source": [
    "# Display sample processed tokens\n",
    "print(\"\\nSample Processed Data:\")\n",
    "print(df[['processed_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Exploratory Data Analysis (EDA)\n",
    "In this section we will analyze the class distribution to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution:\n",
      "cyberbullying_type\n",
      "religion               7965\n",
      "age                    7934\n",
      "ethnicity              7909\n",
      "not_cyberbullying      7852\n",
      "gender                 7728\n",
      "other_cyberbullying    6128\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClass Distribution:\")\n",
    "print(df['cyberbullying_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Frequency:\n",
      "    Token  Frequency\n",
      "0   bully      10601\n",
      "1  school       9108\n",
      "2    fuck       7189\n",
      "3    like       6134\n",
      "4  nigger       5567\n",
      "5     get       5489\n",
      "6    girl       5423\n",
      "7    joke       5414\n",
      "8    dumb       5316\n",
      "9    high       5122\n"
     ]
    }
   ],
   "source": [
    "# Token frequency (flat list of all tokens)\n",
    "from collections import Counter\n",
    "all_tokens = [token for tokens in df['processed_tokens'] for token in tokens]\n",
    "token_counts = Counter(all_tokens).most_common(10)\n",
    "#print as a table\n",
    "print(\"\\nToken Frequency:\")\n",
    "print(pd.DataFrame(token_counts, columns=['Token', 'Frequency']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u should not be a token?? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Feature Engineering\n",
    "Transform processed_tokens into numerical features suitable for model input:\n",
    "Bag-of-Words (BoW). \n",
    "or \n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) ( i belive this is what we selected in our proposal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Modeling\n",
    "train and test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Model Evaluation\n",
    "Compare models using evaluation metrics like:\n",
    "Accuracy.\n",
    "Precision, Recall, and F1-Score.\n",
    "Confusion matrix for deeper insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Stretch Goals (Optional)\n",
    "can cross-compare it with our base model\n",
    "\n",
    "- Sentiment Analysis: Pre-trained sentiment analysis - models (e.g., using TextBlob or VADER) can be applied to processed_text.\n",
    "    - Compare the effectiveness of sentiment-only features against other feature engineering methods (e.g., Bag-of-Words or embeddings).\n",
    "    - Test if sentiment adds value to the regular model's performance.\n",
    "- Intent Detection: Use embeddings or transformer models for intent classification (if time allows)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
